---
title: "DA410_Assignment5_MattGraham"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

This is our first project, analyzing air pollution, mortality rates, and relevant parameters.

```{r Imports, message=FALSE}
library(nnspat)  # used for dist2full()
library("dplyr")  # used to select numeric datatypes
library("ggplot2")
library(reshape)  # used for melting matricies
library(klaR)
library(ggvis)
library(class)
library(gmodels)
library(MASS)
```


## 9.7 - 
### Do a classification analysis on the beetle data in Table 5.5 

Setting up data
```{r}
beetles <- read.table("C:/mattgraham93.github.io/school/22_3_DA410/data/T5_5_FBEETLES.DAT", header=FALSE)
colnames(beetles) <- c("Experiment", 'Species', 'post_dist', 'l_elytra','len_sec_an', 'len_thi_an')

oleracea <- beetles[beetles$Species==1, -c(1:2)]  # we assume that species 1 = oleracea
carduorum <- beetles[beetles$Species==2, -c(1:2)] # we assume that species 2 = carduorum

beetles
```


#### (a) Find the classification function z = (y1 — y~2)'Sp1^-1*y and the cutoff point 1/2(times)(Ίι + z2).
```{r}

cov.ol <- cov(oleracea)
cov.ca <- cov(carduorum)

n1 <- nrow(oleracea)
n2 <- nrow(carduorum)

df1 <- n1 - 1
df2 <- n2 - 1

s.pl <- ( (df1 * cov.ol) + (df2 * cov.ca) ) / (df1 + df2)

as.data.frame(s.pl)
```

Finding column means and z.bar
```{r}

means.ol <- colMeans(oleracea)
means.ca <- colMeans(carduorum)

z.bar <- (means.ol - means.ca) %*% solve(s.pl)

z.ol <- z.bar - means.ol
z.ca <- z.bar - means.ca
```


z.bar
```{r}
as.data.frame(z.bar)
```


z.ol
```{r}
as.data.frame(z.ol)
```


z.ca
```{r}
as.data.frame(z.ca)
```

Finding our cut-off point
```{r}

cutoff <- (z.ol + z.ca) / 2
as.data.frame(cutoff)

```

When looking at our z-bars and associated cut-off point. We can see that all but the post_dist are what primarily distinguish carduorim from olercea beetles!

#### (b) Find the classification table using the linear classification function in part (a).
```{r}

beetle.lda <- lda(Species ~ post_dist + l_elytra + len_sec_an + len_thi_an, data=beetles)
beetle.lda

```

We can see our linear discriminant agree with the sentiment above. We have 3 primary factors that will inherently increase the likelihood that we assign a beetle to class 2, with post_dist being what levels the playing field to provide us with the likelihood of having of class 1.

```{r}
beetle.predict <- predict(beetle.lda, beetles)$class
table(beetles$Species, beetle.predict, dnn = c('Actual group', 'Predicted group'))
```

Error rate:
```{r}
err.rate <- 1 / (n1+n2)
err.rate
```

Overall, we can conclude our model does a great job at predicting a beetle's species within ~2.5%. That's statistically significant, and can be improved!


#### (c) Find the classificaion table using the nearest neighbor method. 
```{r}

set.seed(1234)

beetle.ind <- sample(2, nrow(beetles), replace=TRUE, prob = c(0.67, 0.33))
beetle.training <- beetles[beetle.ind==1, 3:6]
beetle.test <- beetles[beetle.ind==2, 3:6]

beetle.trainingLabels <- beetles[beetle.ind==1,2]
beetle.testLabels <- beetles[beetle.ind==2,2]

beetle.knn.pred <- knn(train=beetle.training, test=beetle.test, cl=beetle.trainingLabels, k=5)

beetleTestLabels <- data.frame(beetle.testLabels)

merge <- data.frame(beetle.knn.pred, beetleTestLabels)
merge

```

Chi-square test
```{r}
CrossTable(x=beetle.testLabels, y=beetle.knn.pred, prop.chisq=FALSE)
```


## 9.12 - 
### Do a classification analysis on the rootstock data of Table 6.2
```{r}
root <- read.table("C:/mattgraham93.github.io/school/22_3_DA410/data/T6_2_ROOT.DAT", header=FALSE)
colnames(root) <- c('Rootstock', 'girth_4', 'growth_4', 'girth_15', 'weight_15')

k <- 6
p <- 4
n <- 8

root
```

#### (a) Find the linear classification functions. 
Finding s.pl
```{r}
root.s.pl <- matrix(0, nrow = p, ncol = p)

for(i in 1:k) {
  root.s.pl <- root.s.pl + cov(root[root$Rootstock == i, 2:5]) / k
}

as.data.frame(root.s.pl)
```

Finding ybar
```{r}
ybar <- matrix(nrow = p, ncol = k)
rownames(ybar) <- colnames(root[, 2:5])
for(i in 1:k) {
  ybar[,i] <- colMeans(root[root$Rootstock == i, 2:5])
}
as.data.frame(ybar)
```

Completing classification function
```{r}
classification <- matrix(nrow = p + 1, ncol = k)
rownames(classification) <- c('c0','c1','c2','c3','c4')

for(i in 1:k) {
  classification[1, i] <- ybar[,i]%*%solve(root.s.pl)%*%as.matrix(ybar[,i]/2)

  classification[2:(p+1), i] <-ybar[,i]%*%solve(root.s.pl)
}
as.data.frame(classification)
```

Above is the complete classification analysis across all rootstocks and measurements. We can see some interesting variability across c3. 


#### (b) Find the classification table using the linear classification functions in part (a) (assuming Σ1 = Σ2 = Σ3). 
```{r}

roots.lda <- lda(Rootstock ~ girth_4 + growth_4 + girth_15 + weight_15, data=root)
roots.lda
```

Observing the diagonals, we can see that our main determining factor is growth at a 4 year period which will decrease the classification determination by a fairly significant factor. This can infer that growth at 4 years is a fairly significant determining factor of how we classify our rootstock.

Getting predictions
```{r}
roots.lda.pred <- predict(roots.lda)$class
table(root$Rootstock, roots.lda.pred, dnn=c("Actual group", "Predicted group"))
```

Defining accuracy
```{r}
correct <- sum(roots.lda.pred==root$Rootstock)
total <- nrow(root)

root.pred.perc <- correct / total
root.pred.err <- 1 - root.pred.perc

root.pred.err
```

We did not produce a very strong model using linear discriminant analysis. This would not be something to use in real life.

#### (c) Find the classification table using quadratic classification functions (assuming population covariance matrices are not equal).

QDA
```{r}
roots.qda <- qda(Rootstock ~ girth_4 + growth_4 + girth_15 + weight_15, data=root)
roots.qda
```
QDA predictions
```{r}
roots.qda.pred <- predict(roots.qda)$class

table(root$Rootstock, roots.qda.pred, dnn = c("Actual group", 'Predicted group'))
```

Defining accuracy
```{r}
correct <- sum(roots.qda.pred==root$Rootstock)
total <- nrow(root)

root.pred.perc <- correct / total
root.pred.err <- 1 - root.pred.perc

root.pred.err
```

Here, we can see a 50% reduction in error, which is significant, but not significant enough. 

#### (d) Find the classification table using the nearest neighbor method. 

```{r}
set.seed(1234)

roots.ind <- sample(2, nrow(root), replace=TRUE, prob = c(0.67, 0.33))
roots.training <- root[roots.ind==1, 2:5]
roots.test <- root[roots.ind==2, 2:5]

roots.trainingLabels <- root[roots.ind==1,2]
roots.testLabels <- root[roots.ind==2,2]

roots.knn.pred <- knn(train=roots.training, test=roots.test, cl=roots.trainingLabels, k=5)

rootsTestLabels <- data.frame(roots.testLabels)

roots.merge <- data.frame(roots.knn.pred, rootsTestLabels)
roots.merge
```

Determining accuracy
```{r}
CrossTable(x=roots.testLabels, y=roots.knn.pred, prop.chisq=FALSE)
```

I don't really know how to analyze this table for accuracy. Ultimately, it looks like we were only able to determine one classification correctly. I must have done something wrong. 


Decision Tree example
```{r}
library(rpart)
library(rpart.plot)

fit <- rpart(Rootstock ~ girth_4 + growth_4 + girth_15 + weight_15, data=root, method='class')
rpart.plot(fit)
```


