source("R_utility_program_4.R") # provides wait-time ribbon plots
load("mtpa_wait_time_ribbon_utility.Rdata")
put.title.on.plots <- TRUE  # put title on wait-time ribbon plots
# The call center data from "Anonymous Bank" in Israel were provided
# by Avi Mandelbaum, with the help of Ilan Guedj.
# data source: http://ie.technion.ac.il/serveng/callcenterdata/index.html
# variable names and definitions from documentation
# VRU  Voice Response Unit automated service
# vru.line  6 digits Each entering phone-call is first routed through a VRU:
#           There are 6 VRUs labeled AA01 to AA06. Each VRU has several lines
#           labeled 1-16. There are a total of 65 lines. Each call is assigned
#           a VRU number and a line number.
# call.id  unique call identifier
# customer.id  unique identifier for existing customer, zero for non-customer
# priority  0 or 1 for unidentified or regular customers
#           2 for priority customers who receive advanced position in queue
# type  type of service
#       PS  regular activity (coded 'PS' for 'Peilut Shotefet')
#       PE  regular activity in English (coded 'PE' for 'Peilut English')
#       IN  internet consulting (coded 'IN' for 'Internet')
#       NE  stock exchange activity (coded 'NE' for 'Niarot Erech')
#       NW  potential customer getting information
#       TT  customers who left a message asking the bank to return their call
#           but, while the system returned their call, the calling-agent became
#           busy hence the customers were put on hold in the queue.
# date  year-month-day
# vru_entry  time that the phone-call enters the call-center or VRU
# vru_exit  time of exit from VRU directly to service or to queue
# vru_time  time in seconds spent in the VRU
#           (calculated by exit_time – entry_time)
# q_start  time of joining the queue (00:00:00 for customers who abandon VRU
#          or do not enter the queue)
# q_exit  time in seconds of exiting queue to receive service or abandonment
# q_time  time spent in queue (calculated by q_exit – q_start)
# outcome  AGENT = service
#          HANG = hang up
#          PHANTOM = a virtual call to be ignored
# ser_start  time of beginning of service by agent
# ser_exit  time of end of service by agent
# ser_time  service duration in seconds (calculated by ser_exit – ser_start)
# server  name of agent, NO_SERVER if no service provided
# focus upon February 1999
call.center.input.data <- read.table("data_anonymous_bank_february.txt",
header = TRUE, colClasses = c("character","integer","numeric",
"integer","character","character","character","character","integer",
"character","character","integer","factor","character","character",
"integer","character"))
# check data frame object and variable values
print(summary(call.center.input.data))
# delete PHANTOM calls
call.center.data <- subset(call.center.input.data, subset = (outcome != "PHANTOM"))
# negative VRU times make no sense... drop these rows from data frame
call.center.data <- subset(call.center.data, subset = (vru_time >= 0))
# calculate wait time as sum of vru_time and q_time
call.center.data$wait_time <-
call.center.data$vru_time + call.center.data$q_time
# define four-digit year so year is not read as 2099
# convert date string to date variable
call.center.data$date <- paste("19", call.center.data$date, sep ="")
call.center.data$date <- ymd(call.center.data$date)
# identify day of the week 1 = Sunday ... 7 = Saturday
call.center.data$day_of_week <- wday(call.center.data$date)
call.center.data$day_of_week <- factor(call.center.data$day_of_week,
levels = c(1:7), labels = c("Sunday","Monday","Tuesday",
"Wednesday","Thursday","Friday","Saturday"))
# examine frequency of calls by day of week
print(table(call.center.data$day_of_week))
# identify the hour of entry into the system
time.list <- strsplit(call.center.data$vru_entry,":")
call.hour <- numeric(nrow(call.center.data))
for (index.for.call in 1:nrow(call.center.data))
call.hour[index.for.call] <- as.numeric(time.list[[index.for.call]][1])
call.center.data$call_hour <- call.hour
# check frequency of calls in February by hour and day of week
print(with(call.center.data, table(day_of_week, call_hour)))
# select first week of February 1999 for data visualization and analysis
# that week began on Monday, February 1 and ended on Sunday, February 7
selected.week <- subset(call.center.data, subset = (date < ymd("19990208")))
sw.df <- selected.week$day_of_week
print(table(sw.df))
# get wait times > 120 minutes
wait.time.120 <- subset(selected.week, selected.week$wait_time > 120)
wt.df <- as.matrix(wait.time.120$day_of_week)
print((table(wt.df) / table(sw.df))*100)
# loop for day of week ignoring Saturdays in Isreal
day.of.week.list <- c("Monday","Tuesday",
"Wednesday","Thursday","Friday","Sunday")
# wait-time ribbon plots for the six selected days
# call upon utility function wait.time.ribbon
# the utility makes use of grid split-plotting
# place ribbon plot and text table/plot on each file
# each plot goes to its own external pdf file
for(index.day in seq(along=day.of.week.list)) {
this.day.of.week <- day.of.week.list[index.day]
pdf(file = paste("fig_operations_management_ribbon_",
tolower(this.day.of.week),".pdf",sep=""), width = 11, height = 8.5)
if(put.title.on.plots) {
ribbon.plot.title <- paste(this.day.of.week,"Call Center Operations")
}
else {
ribbon.plot.title <- ""
}
selected.day <- subset(wait.time.120,
subset = (day_of_week == this.day.of.week),
select = c("call_hour","wait_time","ser_time","server"))
colnames(selected.day) <- c("hour","wait","service","server")
wait.time.ribbon(wait.service.data = selected.day,
title = ribbon.plot.title,
use.text.tagging = TRUE, wait.time.goal = 30, wait.time.max = 90,
plotting.min = 0, plotting.max = 250)
dev.off()
}
# select wait.time.120 in February for the queueing model
thursdays <- subset(call.center.data, subset = (day_of_week == "Thursday"))
# compute arrival rate of calls as calls for hour
# we do not use table() here because some hours could have zero calls
calls.for.hour <- numeric(24)
for(index.for.hour in 1:24) {
# 24-hour clock has first hour coded as zero in input data file
coded.index.for.hour <- index.for.hour - 1
this.hour.calls <-
subset(thursdays, subset = (call_hour == coded.index.for.hour))
if(nrow(this.hour.calls) > 0)
calls.for.hour[index.for.hour] <- nrow(this.hour.calls)
}
# compute arrival rate as average number of calls into VRU per hour
hourly.arrival.rate <- calls.for.hour/4  # four Wednesdays in February
# service times can vary hour-by-hour due to differences
# in service requests and individuals calling hour-by-hour
# begin by selecting calls that receive service
wait.time.120.served <- subset(wait.time.120, subset = (server != "NO_SERVER"))
hourly.mean.service.time <- numeric(24)
served.for.hour <- numeric(24)
for(index.for.hour in 1:24) {
# 24-hour clock has first hour coded as zero in input data file
coded.index.for.hour <- index.for.hour - 1
this.hour.calls <-
subset(wait.time.120.served, subset = (call_hour == coded.index.for.hour))
if(nrow(this.hour.calls) > 0) {
served.for.hour[index.for.hour] <- nrow(this.hour.calls)
hourly.mean.service.time[index.for.hour] <- mean(this.hour.calls$ser_time)
}
}
# hourly service rate given the current numbers of service operators
hourly.served.rate <- served.for.hour/4  # four Wednesdays in February
# build data frame for plotting arrival and service rates
hour <- 1:24  # hour for horizontal axix of line chart
type <- rep("Arrived", length = 24)
value <- hourly.arrival.rate
arrival.data.frame <- data.frame(hour, value, type)
type <- rep("Served", length = 24)
value <- hourly.served.rate
service.data.frame <- data.frame(hour, value, type)
arrival.service.data.frame <- rbind(arrival.data.frame, service.data.frame)
pdf(file = "fig_operations_management_thursday_arrived_served.pdf",
width = 11, height = 8.5)
plotting.object <- ggplot(data = arrival.service.data.frame,
aes(x = hour, y = value, fill = type)) +
geom_line() +
geom_point(size = 4, shape = 21) +
scale_x_continuous(breaks = c(1,3,5,7,9,11,13,15,17,19,21,23,25),
labels =
c("00","02","04","06","08","10","12","14","16","18","20","22","24")) +
theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) +
labs(x = "Hour of Day (24-Hour Clock)", y = "Average Calls per Hour") +
scale_fill_manual(values = c("yellow","dark green"),
guide = guide_legend(title = NULL))  +
theme(legend.position = c(1,1), legend.justification = c(1,1)) +
theme(legend.text = element_text(size=15)) +
coord_fixed(ratio = 1/10)
print(plotting.object)
dev.off()
# examine service times per service operator
# for hours with no service time information use the mean as value
hourly.mean.service.time <-
ifelse((hourly.mean.service.time == 0),
mean(wait.time.120.served$ser_time),
hourly.mean.service.time)
# compute service rate noting that there are 3600 seconds in an hour
# adding 60 seconds to each mean service time for time between calls
# this 60 seconds is the wrap up time or time a service agent remains
# unavailable to answer a new call after a call has been completed
hourly.service.rate <- 3600/(hourly.mean.service.time + 60)
# we observe that mean service times do not vary that much hour-by-hour
# so we use the mean hourly service rate in queueing calculations
# mean(hourly.service.rate) is 14.86443
# so we use 15 calls per hour as the rate for one service operator
SERVICE.RATE <- 15
# C_erlang function from the queueing package
# inputs c = number of servers
#        r = ratio of rate of arrivals and rate of service
# returns the propability of waiting in queue because all servers are busy
# let us set a target for the probability of waiting in queue to be 0.50
# using while-loop iteration we determine the number of servers needed
# we do this for each hour of the day knowing the hourly arrival rate
PROBABILITY.GOAL <- 0.50
servers.needed <- integer(24)  # initialize to zero
for(index.for.hour in 1:24) {
if (hourly.arrival.rate[index.for.hour] > 0) {
erlang.probability <- 1.00  # intialization prior to entering while-loop
while (erlang.probability > PROBABILITY.GOAL) {
servers.needed[index.for.hour] <- servers.needed[index.for.hour] + 1
erlang.probability <- C_erlang(c = servers.needed[index.for.hour],
r = hourly.arrival.rate[index.for.hour]/SERVICE.RATE)
}  # end while-loop for defining servers needed given probability goal
}  # end if-block for hours with calls
}  # end for-loop for the hour
# the result for servers.needed is obtained as
# 1  1  1  0  1  1  1  4  8  9 10  9  8 16 10 10  6  7  8  8  6  6  5  4
# we will assume the bank call center will be closed hours 00 through 05
# but use the other values as the bank's needed numbers of servers
servers.needed[1:6] <- 0
cat("\n","----- Hourly Operator Requirements -----","\n")
print(servers.needed)
# read in case data for the structure of call center worker shifts
bank.shifts.data.frame <- read.csv("data_anonymous_bank_shifts.csv")
# examine the structure of the case data frame
print(str(bank.shifts.data.frame))
constraint.matrix <- as.matrix(bank.shifts.data.frame[,3:10])
cat("\n","----- Call Center Shift Constraint Matrix -----","\n")
print(constraint.matrix)
# six-hour shift salaries in Israeli sheqels
# 1 ILS = 3.61 USD in June 2013
# these go into the objective function for integer programing
# with the objective of minimizing total costs
cost.vector <- data.frame(c(252,288,180,180,180,288,288,288))
call.center.schedule <- lp(const.mat=constraint.matrix,
const.rhs = servers.needed,
const.dir = rep(">=",times=8),
int.vec = 1:8,
objective = cost.vector,
direction = "min")
# prepare summary of the results for the call center problem
ShiftID <- 1:8
StartTime <- c(0,6,8,10,12,2,4,6)
# c("Midnight","6 AM","8 AM","10 AM","Noon","2 PM","4 PM","6 PM")
ShiftDuration <- rep(6,times=8)
HourlyShiftSalary <- c(42,48,30,30,30,48,48,48)
HourlyShiftCost <- call.center.schedule$objective # six x hourly shift salary
Solution <- call.center.schedule$solution
ShiftCost <- call.center.schedule$solution * call.center.schedule$objective
call.center.summary <-
data.frame(ShiftID,StartTime,ShiftDuration,HourlyShiftSalary,
HourlyShiftCost,Solution,ShiftCost)
cat("\n\n","Call Center Summary","\n\n")
print(call.center.summary)
# the solution is obtained by print(call.center.schedule)
# or by summing across the hourly solution times the cost objective
print(call.center.schedule)
cat("\n\n","Call Center Summary Minimum Cost Solution:",sum(ShiftCost),"\n\n")
# build data frame for plotting the solution compared with need
hour <- 1:24  # hour for horizontal axix of line chart
type <- rep("Hourly Need", length = 24)
value <- servers.needed
needs.data.frame <- data.frame(hour, value, type)
type <- rep("Optimal Solution", length = 24)
value <- schedule.fit.to.need <-
constraint.matrix %*% call.center.schedule$solution
solution.data.frame <- data.frame(hour, value, type)
plotting.data.frame <- rbind(needs.data.frame, solution.data.frame)
# plot the solution... solution match to the workforce need
pdf(file = "fig_operations_management_solution.pdf", width = 11, height = 8.5)
heart1("Matt")
# Source: http://statistics.berkeley.edu/classes/s133/heart.r
heart1 = function(name){
t = seq(0,60,len=100)
plot(c(-8,8),c(0,20),type='n',axes=FALSE,xlab='',ylab='')
x = -.01*(-t^2+40*t+1200)*sin(pi*t/180)
y = .01*(-t^2+40*t+1200)*cos(pi*t/180)
lines(x,y, lwd=4)
lines(-x,y, lwd=4)
text(0,7,"Happy Valentine's Day",col='red',cex=2.5)
text(0,5.5,name,col='red',cex=2.5)
}
heart1("Matt")
heart1("Matt!")
write('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', file = "~/.Renviron", append = TRUE)
Sys.which("make")
Sys.which("make")
install.packages("Rstem", repos = "http://www.omegahat.net/R", type = "source")
install.packages("devtools")
install.packages("wordcloud")
install.packages("SnowballC")
install.packages("plyr")
install.packages("ggplot2")
install.packages("RColorBrewer")
install.packages("tm")
require(devtools)
# require(SnowballC)
install_url("https://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.1.tar.gz")
install_url("https://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
library(plyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(tm)
library(SnowballC)
library(sentiment)
textdata = gsub("[[:punct:]]", "", textdata)
data <- readLines("http://www.r-bloggers.com/wp-
content/uploads/2016/01/vent.txt") # from: http://www.wvgazettemail.com/
df <- data.frame(data)
textdata <- df[df$data, ]
textdata = gsub("[[:punct:]]", "", textdata)
textdata
textdata <- df[df$data, ]
###Get the data
data <- readLines("http://www.r-bloggers.com/wp-content/uploads/2016/01/vent.txt")
# from: http://www.wvgazettemail.com/
df <- data.frame(data)
textdata <- df[df$data, ]
textdata
textdata <- df$data
###Get the data
data <- readLines("http://www.r-bloggers.com/wp-content/uploads/2016/01/vent.txt")
# from: http://www.wvgazettemail.com/
df <- data.frame(data)
textdata <- df$data
textdata
textdata <- df$data
textdata = gsub("[[:punct:]]", "", textdata)
textdata = gsub("[[:digit:]]", "", textdata)
textdata = gsub("http\\w+", "", textdata)
textdata = gsub("[ \t]{2,}", "", textdata)
textdata = gsub("^\\s+|\\s+$", "", textdata)
try.error = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
textdata = sapply(textdata, try.error)
textdata = textdata[!is.na(textdata)]
names(textdata) = NULL
textdata
# textdata
```
textdata = gsub("[[:punct:]]", "", textdata)
textdata = gsub("[[:digit:]]", "", textdata)
textdata = gsub("http\\w+", "", textdata)
textdata = gsub("[ \t]{2,}", "", textdata)
textdata = gsub("^\\s+|\\s+$", "", textdata)
try.error = function(x)
{
y = NA
try_error = tryCatch(tolower(x), error=function(e) e)
if (!inherits(try_error, "error"))
y = tolower(x)
return(y)
}
textdata = sapply(textdata, try.error)
textdata = textdata[!is.na(textdata)]
names(textdata) = NULL
# textdata
sent_df = within(sent_df,
emotion <- factor(emotion, levels=names(sort(table(emotion),
decreasing=TRUE))))
class_emo = classify_emotion(textdata, algorithm="bayes", prior=1.0)
emotion = class_emo[,7]
emotion[is.na(emotion)] = "unknown"
class_pol = classify_polarity(textdata, algorithm="bayes")
polarity = class_pol[,4]
sent_df = data.frame(text=textdata, emotion=emotion,
polarity=polarity, stringsAsFactors=FALSE)
sent_df = within(sent_df,
emotion <- factor(emotion, levels=names(sort(table(emotion),
decreasing=TRUE))))
sent_df
sent_df$emotion
sent_df$polarity
sent_df$emotion
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="")
```{r}
ggplot(sent_df, aes(x=polarity)) +
geom_bar(aes(y=..count.., fill=polarity)) +
scale_fill_brewer(palette="RdGy") +
labs(x="polarity categories", y="")
ggplot(sent_df, aes(x=emotion)) +
geom_bar(aes(y=..count.., fill=emotion)) +
scale_fill_brewer(palette="Dark2") +
labs(x="emotion categories", y="")
install.packages("Rstem", repos = "http://www.omegahat.net/R", type = "source")
install.packages("devtools")
install.packages("wordcloud")
install.packages("SnowballC")
install.packages("plyr")
install.packages("ggplot2")
install.packages("RColorBrewer")
install.packages("tm")
require(devtools)
install_url("https://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.1.tar.gz")
install_url("https://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
# install.packages("Rstem", repos = "http://www.omegahat.net/R", type = "source")
# install.packages("devtools")
# install.packages("wordcloud")
# install.packages("SnowballC")
# install.packages("plyr")
# install.packages("ggplot2")
# install.packages("RColorBrewer")
# install.packages("tm")
# require(devtools)
# install_url("https://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.1.tar.gz")
# install_url("https://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
library(plyr)
library(ggplot2)
library(wordcloud)
library(RColorBrewer)
library(tm)
library(SnowballC)
library(sentiment)
emos = levels(factor(sent_df$emotion))
nemo = length(emos)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = textdata[emotion == emos[i]]
emo.docs[i] = paste(tmp, collapse=" ")
}
emo.docs = removeWords(emo.docs, stopwords("english"))
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emos
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE,
title.size = 1.5)
emos = levels(factor(sent_df$emotion))
nemo = length(emos)
emo.docs = rep("", nemo)
for (i in 1:nemo)
{
tmp = textdata[emotion == emos[i]]
emo.docs[i] = paste(tmp, collapse=" ")
}
emo.docs = removeWords(emo.docs, stopwords("english"))
corpus = Corpus(VectorSource(emo.docs))
tdm = TermDocumentMatrix(corpus)
tdm = as.matrix(tdm)
colnames(tdm) = emos
comparison.cloud(tdm, colors = brewer.pal(nemo, "Dark2"),
scale = c(3,.5), random.order = FALSE,
title.size = 1.5)
# install.packages("Rcpp")
# install.packages("aod")
library(aod)
library(ggplot2)
library(Rcpp)
###Get the data
mydata <- read.csv (file="C:/mattgraham93.github.io/school/23_1_DA420/projects/binary.csv")
print(head(mydata))
print(summary(mydata))
print(sapply(mydata, sd))
## we want to make sure there are not 0 cells
xtabs(~ admit + rank, data = mydata)
sapply(mydata, sd)
summary(mydata)
head(mydata)
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
mydata$rank <- factor(mydata$rank)
mylogit <- glm(admit ~ gre + gpa + rank, data = mydata, family = "binomial")
summary(mylogit)
## CIs using profiled log-likelihood
confint(mylogit)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6)
## CIs using standard errors
confint.default(mylogit)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), Terms = 4:6)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), L = l)
l <- cbind(0,0,0,1,-1,0)
wald.test(b = coef(mylogit), Sigma = vcov(mylogit), L = l)
## odds ratios only
exp(coef(mylogit))
# odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))
# calculate predictive probability of admission for each rank
newdata1 <- with(mydata,
data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))
newdata1
newdata1
newdata1$rankP <- predict(mylogit, newdata = newdata1, type = "response")
newdata1
newdata2 <- with(mydata,
data.frame(gre = rep(seq(from = 200, to = 800, length.out = 100), 4),
gpa = mean(gpa), rank = factor(rep(1:4, each = 100))))
newdata2
head(newdata2)
## view first few rows of final dataset
head(newdata3)
newdata3 <- cbind(newdata2, predict(mylogit, newdata = newdata2, type="link",
se=TRUE))
newdata3 <- within(newdata3, {
PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))
})
## view first few rows of final dataset
head(newdata3)
ggplot(newdata3, aes(x = gre, y = PredictedProb)) +
geom_ribbon(aes(ymin = LL, ymax = UL, fill = rank), alpha = .2) +
geom_line(aes(colour = rank), size=1)
with(mylogit, null.deviance - deviance)
with(mylogit, df.null - df.residual)
with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual,
lower.tail = FALSE))
with(mylogit, pchisq(null.deviance - deviance, df.null - df.residual, lower.tail = FALSE))
logLik(mylogit)
logLik(mylogit)
